Hi Rahul, For part 2, btw congratulations on your work well done in part 1! 

Here are some quick updates you can make in refactoring:
	1.	Stabilize the Fitbit Adapter
	•	Add basic error handling (e.g., retries, logging when the device/API fails).
	•	Include a simple data validation step to ensure HRV readings are clean before scoring.
	2.	Simulate Live Data
	•	Even without real-time streaming, create a small script to feed sample HRV data continuously to the pipeline.
	•	This will make it easier to test how the scoring agent reacts under “live” conditions.
	3.	Prepare for Multi-Device Support
	•	Define a basic interface (e.g., DeviceAdapter class) with methods like connect(), fetch_data(), normalize().
	•	Implement Fitbit as the first instance — this will make adding new devices later much easier.

Please asks me if you have any bottleneck :) 

A spec to follow next ➡️ 
Fitbit Adapter Cleanup & Extension – Starter Spec

Context:
Mohammed’s code currently connects to Fitbit and retrieves HRV data, but lacks robustness and modularity. This update will stabilize the adapter, allow simulated streaming, and prepare the codebase for multi-device support.

⸻

1. Stabilize the Fitbit Adapter

Goal: Ensure the adapter handles failures gracefully and only passes clean data to the scoring pipeline.
Tasks:
	•	Add error handling: retries (3x) and logging if the device/API fails.
	•	Implement a data validation step: discard or flag corrupted HRV readings (e.g., missing timestamps, null values).
	•	Confirm the cleaned data is passed in the expected schema (e.g., {timestamp, hrv_value}).

⸻

2. Simulate Live Data

Goal: Enable testing without live Fitbit streaming.
Tasks:
	•	Write a script that continuously feeds sample HRV data (CSV/JSON provided) into the pipeline at intervals (e.g., 1Hz).
	•	Allow toggling between live mode and simulated mode with a flag (e.g., --simulate).

⸻

3. Prepare for Multi-Device Support

Goal: Make it easy to add other wearables later (Apple Watch, Garmin, etc.).
Tasks:
	•	Create a DeviceAdapter interface with methods:
	•	connect()
	•	fetch_data()
	•	normalize()
	•	Refactor the Fitbit adapter to implement this interface as the first instance.
Dropping Testing Criteria
	1.	Adapter logs errors/retries when connection fails.
	2.	Validation filters out invalid HRV readings before scoring.
	3.	Simulation script successfully streams sample data and is recognized by the scoring agent.
	4.	Interface is implemented cleanly, allowing easy addition of new devices later.

What the Vercel app will POST
	•	Endpoint: POST /egc/upload
	•	Accepts: CSV or JSON array of normalized records
	•	Common schema (JSON per row):

{
  "timestamp": "2025-08-17T17:00:00Z",
  "signal": "ecg" | "r_peak" | "st_elev" | "st_depr" | "marked_event",
  "value": 0.845,
  "unit": "mV",
  "meta": { "lead": "I", "sampling_rate_hz": 250, "source": "kids_app" }
}

	•	Sample upload files:
	•	CSV sample (sandbox:/mnt/data/sample_egc_upload.csv?_chatgptios_conversationID=68a20b2a-b844-8327-a31c-a55ef8a6e2c0&_chatgptios_messageID=309f53cf-4244-4096-b32c-81047b267d02)
	•	JSON sample (sandbox:/mnt/data/sample_egc_upload.json?_chatgptios_conversationID=68a20b2a-b844-8327-a31c-a55ef8a6e2c0&_chatgptios_messageID=309f53cf-4244-4096-b32c-81047b267d02)

CSV uses flattened meta keys (meta.lead, meta.sampling_rate_hz, meta.source, meta.label). Either format is fine.

Ingest/normalize rules
	1.	Assume timestamps are UTC (allow optional tz_offset override on upload).
	2.	Drop empty/NaN rows; sort by timestamp; de-dup by (timestamp, signal).
	3.	Persist raw, plus a normalized copy in a single table/collection.

What your GET should return
	•	Events:
	•	GET /egc/events?since=2025-08-17T16:59:00Z&until=2025-08-17T17:00:06Z
	•	Sample response: JSON (sandbox:/mnt/data/sample_get_egc_events_response.json?_chatgptios_conversationID=68a20b2a-b844-8327-a31c-a55ef8a6e2c0&_chatgptios_messageID=309f53cf-4244-4096-b32c-81047b267d02)

{
  "since": "2025-08-17T16:59:00Z",
  "until": "2025-08-17T17:00:06Z",
  "count": 3,
  "events": [
    { "timestamp": "2025-08-17T17:00:02Z", "type": "r_peak", "hr_estimate_bpm": 72, "source": "kids_app" },
    { "timestamp": "2025-08-17T17:00:03.5Z", "type": "st_elev", "magnitude_mv": 1.6, "lead": "I", "source": "kids_app" },
    { "timestamp": "2025-08-17T17:00:04.2Z", "type": "marked_event", "label": "button_press", "source": "kids_app" }
  ]
}

	•	Optional waveform window:
	•	GET /egc/waveform?window=2025-08-17T17:00:00Z/2025-08-17T17:00:06Z&lead=I
	•	Returns time-ordered ecg points (mV) and any inline event markers.

Tests to add
	•	CSV + JSON happy paths (mixed signals + meta).
	•	Malformed rows (bad timestamps, unknown signal, non-numeric value where required).
	•	De-dup + sort behavior.
	•	Summary counters: rows_ingested, rows_dropped, events_emitted.

Notes
	•	Keep auth/session out of the upload path for now (files are push data), but wire your usual observability.
	•	Document the schema in README + a tiny mapping table (e.g., “R-peak → r_peak”).
	•	Devices stay pluggable later (chest strap, watch, etc.) without breaking consumers.


A few notes on polish and next steps:
	•	Fixtures & reuse: Right now the session setup and token payloads repeat across tests. Moving those into conftest.py would make the suite cleaner and easier to maintain.
	•	Edge cases: Good coverage of the basics — let’s add a few more:
	•	Empty HRV dataset
	•	All malformed HRV entries
	•	Token response missing required fields
	•	Expired refresh token (not just access token)
	•	429 rate limit response
	•	Assertions: For callback success, let’s also assert that tokens and expiration are properly persisted in session, not just that the route returned success.
	•	Parametrization: Some of the malformed HRV cases could be collapsed into one parametrized test to cut down repetition.
	•	Time handling: Using a fixed clock (freezegun or a monkeypatch) will make expiry logic deterministic and avoid flakiness in CI.